---
title:  "스크래피를 사용한 크롤링(1)"
excerpt: "파이썬으로 만들어진 대표적인 크롤러 : 스크래피"
categories:
  - Python
tags:
  - WebCrawling, Scrapy, Python, SelfStudy
---

### 스크래피의 장정
1. 스크랩할 항목 유형별 클래스를 만들 수 있음
2. 수집한 데이터를 원하는 대로 편집이 가능
3. 서버에 연동하기 위해 기능을 확장할 수 있음
4. 크롤링 결과를 JSON, XML, CSV 등의 형식으로 내보낼 수 있음
5. 손상된 HTML파일을 분석할 수 있음
-----------------------------------------------
## 1.스크래피 설치하하기
### 1-1. "pip install scrapy" 명령
- Termianl창에 위와 같은 명령을 실행한다.
- 설치가 완료된지 확인을 위해 "scrapy"명령을 한 번 더 실행시킨다.
  아래와 같이 나온다면 설치가 완료된 것

<img src="https://user-images.githubusercontent.com/87592790/189274567-085348de-4828-436d-ab9a-1a5cd3115451.png" width="500" height="300">

## 2.스크래피 프로젝트 생성 : startproject
### 2-1. "scrapy startproject [프로젝트이름]"
- Terminal에 "scrapy startproject [프로젝트이름]"명령을 실행
- 명령을 실행한 경로에 새로운 프로젝트가 생성된다. 
- 한빛미디어 홈페이지에 책을 크로릴할 예정으로 hanbit_crawling이라고 지었음

<img src="https://user-images.githubusercontent.com/87592790/189276099-6e504380-f9e6-45b8-87da-7810350a1394.png" width="600" height="100">

<img src="https://user-images.githubusercontent.com/87592790/189276199-e5ab508c-58f7-4c8d-9b52-22750aa67826.png" width="300" height="300">

## 3.아이템 설정하기
- 크롤링하는 이유는 기본적으로 비정형 데이터인 웹페이지를 목적에 맞게 일정한 형태로 가공하기 위해 사용할 것이다
- 프로제트 안에 item.py의 수정을 통해 가져올 정보를 설정한다.

#### 3-1. [ 크롤링할_할목_이름 = scrapy.Field() ] 형식으로 아이템을 설정
```python
class HanbitCrawlingItem(scrapy.Item):
    
    # 책 이름
    book_title = scrapy.Field()
    
    # 저자 이름
    book_author = scrapy.Field()
    
    # 번역자 이름
    book_translator = scrapy.Field()
    
    # 출간일
    book_pub_date = scrapy.Field()
    
    # ISBN
    book_isbn = scrapy.Field()
    pass
```

## 4.스파이더 만들기
### 4-1. "scrapy genspider" 명령으로 스파이더 생성
- Terminal에 "scrapy genspider"명령을 실행 / 해당 프로젝트 안에서 실행할 것
- 실행 후 화면 및 폴더는 아래와 같음

<img src="https://user-images.githubusercontent.com/87592790/189280476-1e4f588f-dc47-46e0-87b1-122d40e878f3.png" width="450" height="550">  <img src="https://user-images.githubusercontent.com/87592790/189280395-944b8b74-bb55-41a0-a381-aaa2a7966b48.png" width="300" height="400">

### 4-2. "scrapy genspider -t [생성하고싶은_파일_이름] [크롤링할_주소]" 크롤링할 스파이더 파일 생성하기
- Terminal에 "scrapy genspider -t crawl book_crawl hanbit.co.kr" 실행
- 아래와 같이 생성됨

<img src="https://user-images.githubusercontent.com/87592790/189281846-bf2f4574-51f4-49ce-a74b-eeb845cdff8d.png" width="800" height="70">
<img src="https://user-images.githubusercontent.com/87592790/189281845-c1ee92e1-a0de-483f-893d-70bdc81c6256.png" width="250" height="270">
<img src="https://user-images.githubusercontent.com/87592790/189281843-3691dcfd-6913-4362-9c8d-efeafdd4f5da.png" width="800" height="600">

## 5.스파이더 파일 






