---
title:  "스크래피를 사용한 크롤링(1)"
excerpt: "파이썬으로 만들어진 대표적인 크롤러 : 스크래피"
categories:
  - Python
tags:
  - WebCrawling, Scrapy, Python, SelfStudy
---

### 스크래피의 장정
1. 스크랩할 항목 유형별 클래스를 만들 수 있음
2. 수집한 데이터를 원하는 대로 편집이 가능
3. 서버에 연동하기 위해 기능을 확장할 수 있음
4. 크롤링 결과를 JSON, XML, CSV 등의 형식으로 내보낼 수 있음
5. 손상된 HTML파일을 분석할 수 있음

-----------------------------------------------
## 1.스크래피 설치하하기
### 1-1. "pip install scrapy" 명령
- Termianl창에 위와 같은 명령을 실행한다.
- 설치가 완료된지 확인을 위해 "scrapy"명령을 한 번 더 실행시킨다.
  아래와 같이 나온다면 설치가 완료된 것

  <img src="https://user-images.githubusercontent.com/87592790/189274567-085348de-4828-436d-ab9a-1a5cd3115451.png" width="500" height="300">

## 2.스크래피 프로젝트 생성 : startproject
### 2-1. "scrapy startproject [프로젝트이름]"
- Terminal에 "scrapy startproject [프로젝트이름]"명령을 실행
- 명령을 실행한 경로에 새로운 프로젝트가 생성된다. 
- 한빛미디어 홈페이지에 책을 크로릴할 예정으로 hanbit_crawling이라고 지었음

  <img src="https://user-images.githubusercontent.com/87592790/189276099-6e504380-f9e6-45b8-87da-7810350a1394.png" width="300" height="300">

  <img src="https://user-images.githubusercontent.com/87592790/189276199-e5ab508c-58f7-4c8d-9b52-22750aa67826.png" width="200" height="200">

## 3.아이템 설정하기
- 크롤링하는 이유는 기본적으로 비정형 데이터인 웹페이지를 목적에 맞게 일정한 형태로 가공하기 위해 사용할 것이다
- 프로제트 안에 item.py의 수정을 통해 가져올 정보를 설정한다.

#### 3-1. [ 크롤링할_할목_이름 = scrapy.Field() ] 형식으로 아이템을 설정
```python
class HanbitCrawlingItem(scrapy.Item):
    
    # 책 이름
    book_title = scrapy.Field()
    
    # 저자 이름
    book_author = scrapy.Field()
    
    # 번역자 이름
    book_translator = scrapy.Field()
    
    # 출간일
    book_pub_date = scrapy.Field()
    
    # ISBN
    book_isbn = scrapy.Field()
    pass
```

## 4.스파이더 만들기
### 4-1. "scrapy genspider" 명령으로 스파이더 생성
- Terminal에 "scrapy genspider"명령을 실행 / 해당 프로젝트 안에서 실행할 것
- 실행 후 화면 및 폴더는 아래와 같음

  <img src="https://user-images.githubusercontent.com/87592790/189280476-1e4f588f-dc47-46e0-87b1-122d40e878f3.png" width="350" height="450">  <img src="https://user-images.githubusercontent.com/87592790/189280395-944b8b74-bb55-41a0-a381-aaa2a7966b48.png" width="200" height="300">

### 4-2. "scrapy genspider -t [생성하고싶은_파일_이름] [크롤링할_주소]" 크롤링할 스파이더 파일 생성하기
- Terminal에 "scrapy genspider -t crawl book_crawl hanbit.co.kr" 실행
- 아래와 같이 생성됨

  <img src="https://user-images.githubusercontent.com/87592790/189281846-bf2f4574-51f4-49ce-a74b-eeb845cdff8d.png" width="800" height="70">
  
  <img src="https://user-images.githubusercontent.com/87592790/189281845-c1ee92e1-a0de-483f-893d-70bdc81c6256.png" width="250" height="270">
  
  <img src="https://user-images.githubusercontent.com/87592790/189281843-3691dcfd-6913-4362-9c8d-efeafdd4f5da.png" width="800" height="600">

## 5.스파이더 파일 수정하기
```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class BookCrawlSpider(CrawlSpider):
    # 크롤러의 이름
    name = 'book_crawl'
    
    # 크롤러 실행을 허용할 도메인 / 허용된 도메인 이외는 무시함
    allowed_domains = ['hanbit.co.kr']
    
    # 시작점으로 사용할 url / 리스트로 지정해 한 번에 여러 웹 페이지에서 크롤링을 시작하게 할 수 있음
    start_urls = [
        'http://www.hanbit.co.kr/store/books/category_list.html?cate_cd=001001',
        # 'http://www.hanbit.co.kr/store/books/category_list.html?cate_cd=002',
                  ]
    # 크롤러가 어떻게 작동할지 규칙을 설정하는 단계
    # 시작점의 모든 링크를 검사한 후, 규칙에 맞는 링크가 있으면 정해진 콜백 메서드를 실행
    rules = (
        Rule(
            # store/books/look.php?p_code=B8463790401
            LinkExtractor(allow=r'store/books/look.php\?p_code=.*'),  

            # 해당 링크에 요청을 보내고 응답이 오면 실행할 콜백 메서드를 지정
            callback='parse_item', 
            
            # True로 설정되어 있으면, 응답에 다시 한번 rules를 적용해 재귀적으로 실행합니다.
            follow=False),
        
        # 이렇게 여러 개의 규칙을 설정 가능
        # Rule(LinkExtractor(allow=r'store/books/category_list\.html\?page=\d+&cate_cd=00\d+&srt=p_pub_date')))
#=================================================================================================
    def parse_item(self, response):
        """rules를 통과한 링크에 요청을 보내 응답을 받으면 Rule()에 설정한 콜백 메서드를 해당 
           응답 결과에 실행합니다. 따라서 response를 파라미터로 받고 XPath라든가 CSS 선택자를 이용해서 원하는 요소를 추출할 수 있습니다.
        """

        i = {}
        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
        #i['name'] = response.xpath('//div[@id="name"]').extract()
        #i['description'] = response.xpath('//div[@id="description"]').extract()
        
        # 책 이름
        i['book_title'] = response.xpath('//*[@id="container"]/div[1]/div[1]/div[1]/div[2]/h3/text()').extract()
        
        # 저자 이름
        i['book_author'] = response.xpath('//*[@id="container"]/div[1]/div[1]/div[1]/div[2]/ul/li[strong/text()="저자 : "]/span/text()').extract()
        
        # 번역자 이름
        i['book_translator'] = response.xpath('//*[@id="container"]/div[1]/div[1]/div[1]/div[2]/ul/li[strong/text()="번역 : "]/span/text()').extract()
        
        # 출간일
        i['book_pub_date'] = response.xpath('//*[@id="container"]/div[1]/div[1]/div[1]/div[2]/ul/li[strong/text()="출간 : "]/span/text()').extract()

        # ISBN
        i['book_isbn'] = response.xpath('//*[@id="container"]/div[1]/div[1]/div[1]/div[2]/ul/li[strong/text()="ISBN : "]/span/text()').extract()
        return i
```
### 5-1. 스파이더 파일의 파서함수(parse_item) 정의하기
- 파서함수 : 원하는 데이터를 HTML코드 안에서 Xpath나 CSS를 이용해서 추출함
  (1) 웹브라우저의 '개발자 모드'로 HTML 코드 열기
  <img src="https://user-images.githubusercontent.com/87592790/189473402-9b24b45f-147b-42af-b57a-51dfda8ce1d9.png" width="800" height="500">
  
  (2) 개발자 모드 창에 'Element' 왼쪽에 원하는 정보의 코드를 볼 수 있는 '화살표'모양을 클릭하여 활성화한 후
      홈페이지 내에서 내가 가져오고 싶은 정보에 가져다 놓으면 해당하는 HTML을 찾지않고 볼 수 있다.
  <img src="https://user-images.githubusercontent.com/87592790/189473477-38120ce0-a006-43c9-8139-5be51cb4d172.png" width="800" height="500">
  
  (3) 해당 HTML에 마우스를 올려놓고 오른쪽 클릭 - Copy - Copy Xpath 순으로 Xpth를 쉽게 복사할 수 있다.
  <img src="https://user-images.githubusercontent.com/87592790/189473585-0d5f822e-5591-4bee-9dbb-22a94e005c0c.png" width="800" height="500">
  
  (4) i['[아이템_이름]'] = response.xpath('[Xpath경로]').extract() 형식으로 입력해서 작동시킨다.
  
## 6.Scrapy 작동 시키기
### 6-1. Terminal에 "scrapy crawl [스파이더파일_이름] -o [추출된_정보가_저장되는_파일_이름].csv -t csv  
* 지원하는 파일 포맷 : jsonlines, json, xml, csv, pickle, jl, marshal
 <img src="https://user-images.githubusercontent.com/87592790/189473862-6f985d0e-9b45-469e-ab34-584510efdfc0.png" width="600" height="50">

 <img src="https://user-images.githubusercontent.com/87592790/189473865-dd87f462-9ad1-43ac-b0ee-01c9f388f2f3.png" width="600" height="100">
